# 監視データソース

選択した監視システムは、使用する監視データの特定のソースによって通知されます。
このセクションでは、監視データの2つの一般的なソースであるログとメトリクスについて説明します。
[分散トレーシング](http://bit.ly/2syvpOw)やランタイムイントロスペクションなど、ここでは取り上げない貴重な監視ソースもあります。

メトリクスとは、通常は一定の時間間隔で多数のデータ・ポイントを介して収集される属性とイベントを表す数値測定値です。
ログは、イベントの追加専用のレコードです。
この章では、プレーンテキストログではなく、リッチクエリおよび集約ツールを可能にする構造化ログに焦点を当てます。

Googleのログベースシステムは、細分性の高い大量のデータを処理します。
イベントが発生してからログに記録されるまでには、固有の遅延があります。
時間に依存しない分析の場合、これらのログはバッチシステムで処理し、アドホッククエリで問い合わせて、ダッシュボードで視覚化できます。
このワークフローの例としては、ログ処理に[Cloud Dataflow](https://cloud.google.com/dataflow/)、アドホッククエリに[BigQuery](https://cloud.google.com/bigquery/)、ダッシュボードに[Data Studio](https://datastudio.google.com/)を使用することが考えられます。

対照的に、私たちのメトリクスベースの監視システムは、Googleのすべてのサービスから多くのメトリクスを収集しますが、情報の粒度はずっと低く、ほぼリアルタイムで情報を提供します。
これらの特性は、リアルタイムログシステムや高濃度メトリクスなどの例外はありますが、ログとメトリクスに基づく他の監視システムではかなり典型的です。

通常、アラートとダッシュボードはメトリクスを使用します。
当社のメトリクスベースの監視システムのリアルタイム性は、エンジニアに非常に迅速に問題を通知できることを意味します。
必要な情報がメトリックとして使用できないことが多いため、問題の根本原因を見つけるためにログを使用する傾向があります。

レポーティングに時間がかからない場合は、ログ処理システムを使用して詳細なレポートを生成することがよくあります。これは、ログはほとんどの場合、メトリクスよりも正確なデータを生成するためです。

メトリクスに基づいてアラートを生成する場合は、ログに基づいてさらにアラートを生成したくなることがあります。
例えば、例外イベントが1つでも発生した場合に通知を受け取る必要がある場合などです。
特定のイベントが発生したときにカウンターメトリックを増加させ、そのメトリックの値に基づいてアラートを設定できる場合は、引き続きメトリクスベースのアラートを使用することをお勧めします。
この方法では、すべてのアラート設定が一箇所に保持されるため、管理が容易になります([67ページの「監視システムの管理」](../04_03_managing-your-monitoring-system/README.md)を参照してください)。

## 例

以下の実際の例は、監視システム間の選択プロセスを通して推論する方法を示しています。

### ログからメトリクスへ情報を移動する

**課題** HTTPステータス・コードは、エラーをデバッグするApp Engineのカスタマーにとって重要なシグナルです。
この情報はログでは使用できますが、メトリクスでは使用できません。
メトリクス・ダッシュボードには、全エラーの全体レートのみが表示され、正確なエラー・コードやエラー原因に関する情報は含まれていませんでした。
その結果、問題をデバッグするワークフローは次のようになります。

1. 全体エラー・グラフを参照して、エラーが発生した時刻を調べます。
1. ログファイルを読み込んで、エラーを含む行を探します。
1. ログファイル内のエラーをグラフに関連付けます。

ロギング・ツールにはスケール感がないため、1つのログ行で見られるエラーが頻繁に発生しているかどうかを判断するのは困難です。
ログには他にも多くの無関係な行が含まれているため、根本原因を突き止めるのは困難です。

**考えられるソリューション** App Engine開発チームは、HTTPステータス・コードをメトリックのラベルとしてエクスポートすることにしました(例:requests_total{ステータス=404}とrequests_total{ステータス=500})。
異なるHTTPステータス・コードの数は比較的限られているため、メトリック・データの量が非実用的なサイズに増加することはありませんでしたが、最も適切なデータをグラフ化およびアラートに使用できるようになりました。

**結果** この新しいラベルにより、チームはグラフをアップグレードして、エラーのカテゴリとタイプごとに別々の行を表示できるようになりました。
カスタマーは、表示されたエラー・コードに基づいて、考えられる問題についてすばやく推測できます。
また、クライアントとサーバのエラーに対して異なるアラートしきい値を設定し、より正確にアラートをトリガーできるようになりました。

### ログとメトリクスの両方を改善する

**課題** ある広告SREチームは、さまざまな言語やフレームワークで書かれた約50のサービスを維持していた。
チームは、ログをSLO準拠の標準的な情報源として使用しました。
エラーバジェットを計算するために、各サービスは多くのサービス固有の特殊ケースを持つログ処理スクリプトを使用しました。
1つのサービスのログエントリを処理するスクリプトの例を次に示します。

If the HTTP status code was in the range (500, 599)  
AND the 'SERVER ERROR' field of the log is populated  
AND DEBUG cookie was not set as part of the request  
AND the url did not contain '/reports'  
AND the 'exception' field did not contain 'com.google.ads.PasswordException'  
THEN increment the error counter by 1

これらのスクリプトは管理が困難であり、メトリクスベースの監視システムでは利用できないデータも使用していました。
メトリクスによってアラートが生成されるため、アラートはユーザが直面するエラーに対応しない場合があります。
すべてのアラートで、ユーザが直面しているかどうかを判断するための明確な優先順位付け手順が必要となり、レスポンスタイムが遅くなりました。

**考えられるソリューション** チームは、各アプリケーションのフレームワーク言語のロジックに接続するライブラリを作成しました。
ライブラリは、リクエスト時にエラーがユーザに影響を与えているかどうかを判断しました。
計装はこの判断をログに書き込み、同時にメトリックとしてエクスポートして一貫性を向上させました。
メトリックでサービスがエラーを返したことが示された場合、ログには正確なエラーが含まれ、問題の再現とデバッグに役立つリクエスト関連データも含まれていました。
同様に、SLOに影響を与えるエラーがログに記録されると、SLIメトリクスも変更され、チームはアラートを発することができます。

**結果** 複数のサービスにわたって統一されたコントロール面を導入することで、チームは複数のカスタムソリューションを実装する代わりに、ツールと警告ロジックを再利用しました。
すべてのサービスは、複雑なサービス固有のログ処理コードを削除することで、拡張性を向上させることができました。
アラートがSLOに直接関連づけられるようになると、対応がより明確になり誤検出率が大幅に低下しました。

### ログをデータソースのままにする

**課題** 本番環境の問題を調査する際、SREチームが影響を受けるエンティティIDを調べて、ユーザへの影響と根本原因を特定することがよくありました。
先ほどのApp Engineの例と同じく、この調査にはログでしか入手できないデータが必要でした。
インシデントに対応している間に、チームはこのために1回限りのログクエリを実行する必要がありました。
このステップにより、インシデントのリカバリに時間がかかりました。数分でクエリを正しく作成し、さらにログに問い合わせを行う時間が必要になりました。
